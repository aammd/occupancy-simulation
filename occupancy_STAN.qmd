---
title: "Simple occupancy STAN"
author: "Andrew MacDonald and Gabriel Bergeron"
format: 
  html:
    page-layout: article
editor: source
date: today
toc: true
toc-location: left
number-sections: true
number-depth: 3
comments:
  hypothesis: true
editor_options: 
  chunk_output_type: console
---


```{r setup, eval=TRUE, message=FALSE, warning=FALSE}
#| echo: false

library(ggplot2)
library(tidyverse)
library(targets)
library(stantargets)

options(tidyverse.quiet = TRUE)

set.seed(2)
```


# Very simple occupancy model that controls for effort

We begin with a simple occupancy model where the probability of observing a presence ($P$) is a Bernoulli process described by $w$ the probability of observation and $p$ the probability of a true presence. 

$$
\begin{align}
Pr(y = 1) &= Bernoulli(wp) \\
w &= 1 - (1 - d)^{effort} \\\\ 
logit(p) &= \alpha \\
logit(d) &= \beta \\\\
\alpha &\sim \text{N}(-1, 0.5) \\
\beta &\sim \text{N}(0, 0.5) \\\\
effort &= \text{Nb observers} * \text{Nb hours} 
\end{align}
$$

The curve of the effect of effort on probability of observation looks like this (here for d = 0.3)

```{r}
#| echo: false

curve(1 - (1 - 0.3)^x, xlim = c(0, 25), xlab = "Number of effort unit", ylab = "probability of observation")
abline(v = 1, h = 0.3, lty = 2)
```

The probability of observing a presence ($y = 1$) is the product of the probability of observing the specie and the probability that the specie is really there. The probability of not observing a presence is the probability of missing the species while it is present AND the probability that the specie is absent. 


$$
y = 
\begin{cases}
y = 1, & wp \\
y = 0, & (1-w)p + 1 - p
\end{cases}
$$

## Simple Stan model for a static set of parameters

Parameters:
p = 0.7
d = 0.3


We create a fake data set in which we consider the effect of the observation effort in the probability of really observing the specie. We consider an uniform distribution of the effort bounded between 1 and 25. In the real data set, the effort follow a normal distibution centered around 8 with min and max of (~0.5 ; ~ 20).

Here his an example of the data used to run the model :
```{r}
#| echo: false

tar_load(fake_data)
fake_data
```
### Simulation results
From there we ran a Bayesian model (8 batches, 4 reps) and compare the distribution of the posteriors with the real parameters. 
First, we ask what percentage of the posterior don't have the real parameters values in their 95% quantile.

```{r}
#| echo: false

tar_load(cov_params)
cov_params
```

Then we check the distribution of the mean of the posteriors

```{r}
#| echo: false
#| layout-ncol: 2
#| column: page
#| warning: false

tar_load(simple_occ)

simple_occ |>
  filter(variable == "prob_detect") |>
  ggplot(aes(x = mean)) +
  geom_histogram() +
  geom_vline(xintercept = unique(0.3)) +
  ggtitle("Probability of detection") +
  xlim(0, 1) +
  theme_classic()

simple_occ |>
  filter(variable == "prob_pres") |>
  ggplot(aes(x = mean)) +
  geom_histogram() +
  geom_vline(xintercept = unique(0.7)) +
  ggtitle("Probability of presence") +
  xlim(0, 1) +
  theme_classic()

```


## Simple Stan model for a varying set of parameters

Here, we ran each simulation with a different set of parameters. We aim to see if the model is robust for all the parameters value possible. The structure of the data is the same as for the static simulation.

The probability distributions of the parameters look like:

**Detection**

We expect the detection to be centered lower that 0.5 as most species are not systematly detected on each observation. It's is less probable that as species has a probability of detection very close to zero (we would not see it often).
```{r}
#| echo: false
#| fig-height: 4
x <- seq(0, 1, length.out = 100)

plot(dbeta(x, 2, 5) ~ x, type = "l",
     ylab = "density", xlab = "param value")
```

**Presence**

We test for mean value of presence as we expect that in a data set that ignore time, about half of the observations will fall outside the presence period of the species.
```{r}
#| echo: false
#| fig-height: 4
plot(dbeta(x, 2, 2) ~ x, type = "l",
     ylab = "density", xlab = "param value")
```

### Simulation results

From there we ran a Bayesian model (16 batches, 4 reps) and compare the distribution of the posteriors with the real parameters. 
First, we ask what percentage of the posterior have the real parameters values in their 95% quantile.

```{r}
#| echo: false

tar_load(cov_vary_params)
cov_vary_params
```

Then we check the correlation with the mean of the posterior with the true parameter value on a 1:1 plot

```{r}
#| echo: false
#| layout-ncol: 2
#| column: page
#| warning: false

tar_load(vary_params)

vary_params |>
  filter(variable == "prob_detect") |>
  ggplot(aes(y = mean, x = .join_data)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype="dashed") +
  ggtitle("Probability of detection") +
  xlim(0, 0.75) + ylim(0, 0.75) +
  theme_classic()

vary_params |>
  filter(variable == "prob_pres") |>
  ggplot(aes(y = mean, x = .join_data)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype="dashed") +
  ggtitle("Probability of presence") +
  xlim(0, 1) + ylim(0, 1) +
  theme_classic()
```

# Occupancy model that considers time

We build on the preceding model structure to add variation in the presence of the species with time. As we follow migrant species onsite, we expect to detect the arrival and departure of the species in the data. To account for that, we modify the $P$ so that :

$$
\begin{align}
p = \frac{1}{1 + e^{-a_1(t - b_1)}} * \frac{1}{1 + e^{a_2(t - b_2)}} 
\end{align}
$$

Here is an exemple of the probability distribution obtained through time for given parameters value :
```{r}
# Setting parameter values
a1 <- 1
a2 <- 0.2
b1 <- 125
b2 <- 235

```

```{r}
#| echo: false
Hlogist <- function(x, a1, a2, b1, b2) ((1 / (1 +  exp(-a1 * (x - b1)))) * 1 / (1 +  exp(a2 * (x - b2))))

curve(Hlogist(x, a1, a2, b1, b2), xlim = c(110, 270), 
      lwd = 2,
      ylab = "p(x)",
      xlab = "julian date")
abline(v = c(b1, b2), lty = 2)

```

## Simple Stan model for static set of parameters

Parameters: 
d = 0.3
a1 = 1
a2 = 0.2
b1 = 160
b1 = 200

We create a fake data se with fixed parameters. We consider an uniform distribution of the observation dates between jj 130 and 240. In the real data set, the date follow a normal distribution centered around jj 180 (min 130 and max 240).

Here is an example of the data used to run the model: 

```{r}
#| echo: false

tar_load(fake_data_time)
fake_data_time
```

### Small detour on log scale

We change the general equation so that the $a_1$ and $a_2$ parameters are set on the log scale (so in the equation it becomes $exp(a_1)$). We do this because it makes the sampling easier. The $a$ parameters is bounded between 0 and infinite, but starting at a value of 1, the slope of the curvature of the function tend toward a "step". Thus, sampling the parameters on the log scale allows more "weight" on those values below 1, while still allowing the sampling of higher value.

Our priors become: 

log_a1 ~ normal(1.5, 2)

log_a2 ~ normal(1.5, 2)

### Simulation results

From there we ran a Bayesian model (8 batches, 5 reps) and compare the distribution of the posteriors with the real parameters. 
First, we ask what percentage of the posterior don't have the real parameters values in their 95% quantile.

```{r}
#| echo: false

tar_load(cov_fixed_eff_time)
cov_fixed_eff_time
```

Then we check the distribution of the mean of the posteriors

```{r}
#| echo: false
#| column: page
#| warning: false

tar_load(fixed_eff_time)

fixed_eff_time |>
  filter(variable == "prob_detect") |>
  ggplot(aes(x = mean)) +
  geom_histogram() +
  geom_vline(xintercept = unique(0.3)) +
  ggtitle("Probability of detection") +
  xlim(0, 1) +
  theme_classic()

```

```{r}
#| echo: false
#| layout-ncol: 2
#| column: page
#| warning: false

fixed_eff_time |>
  filter(variable == "log_a1") |>
  ggplot(aes(x = mean)) +
  geom_histogram() +
  geom_vline(xintercept = log(1)) +
  ggtitle("log_a1") +
  xlim(-2, 2) + 
  theme_classic()

fixed_eff_time |>
  filter(variable == "log_a2") |>
  ggplot(aes(x = mean)) +
  geom_histogram() +
  geom_vline(xintercept = log(0.2)) +
  ggtitle("log_a2") +
  xlim(-3, 1) + 
  theme_classic()

fixed_eff_time |>
  filter(variable == "b1") |>
  ggplot(aes(x = mean)) +
  geom_histogram() +
  geom_vline(xintercept = 160) +
  ggtitle("b1") +
  xlim(100, 200) + 
  theme_classic()

fixed_eff_time |>
  filter(variable == "b2") |>
  ggplot(aes(x = mean)) +
  geom_histogram() +
  geom_vline(xintercept = 200) +
  ggtitle("b2") +
  xlim(150, 250) + 
  theme_classic()

```


## Simple Stan model for varying set of parameters


**Warning** 

*I changed back to a uniform distribution of the arrival date and departure date to make sure to sample along a wider range of values.*

**Warning**

In this simulation, we sample a different set of parameters each time we run the model. The <span style="color:red;">**arrival date $b_1$**</span> is normally distributed around jj 150 and the <span style="color:blue;">**departure date $b_1$**</span> is distributed around 220. Both have an sd of 7. 

```{r}
#| echo: false
#| fig-height: 4
x1 <- seq(120, 180, length.out = 100)
x2 <- seq(190, 250, length.out = 100)

plot(
  dnorm(x1, 150, 7) ~ x1,
  type = "l",
  ylab = "density",
  xlab = "jj date",
  col = "red",
  xlim = c(120, 250),
  lwd = 2
)
lines(
  dnorm(x2, 220, 7) ~ x2,
  type = "l",
  col = "blue",
  lwd = 2
)
```

The structure of the data and the model is the same than with the fixed parameter analysis.

### Simulation results

We run ran a Bayesian model (x batches, y reps) and compared the distribution of the posteriors with the real parameters. As before, we first look at the percentage of the posterior that include the real parameter in their 95% quantile. 

```{r}
#| echo: false

tar_load(cov_vary_time_params)
cov_vary_time_params
```


Then we check the correlation with the mean of the posterior and the true parameters value with a 1:1 plot.

```{r}
#| echo: false
#| column: page
#| warning: false

tar_load(vary_eff_time)

vary_eff_time |>
  filter(variable == "prob_detect") |>
  ggplot(aes(y = mean, x = .join_data)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype="dashed") +
  ggtitle("Probability of detection") +
  xlim(0, 0.75) + ylim(0, 0.75) +
  theme_classic()

```

```{r}
#| echo: false
#| layout-ncol: 2
#| column: page
#| warning: false

# Create density probability matrix for the sampling date distribution
dates <- c(110:250)
m.unif <- expand.grid(dates, dates)
names(m.unif) <- c(".join_data", "mean")
m.unif$densities <- dunif(m.unif$.join_data, 130, 240) 

vary_eff_time |>
  filter(variable == "log_a1") |>
  ggplot(aes(y = mean, x = .join_data)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype="dashed") +
  ggtitle("log a1") +
  xlim(-3, 1) + ylim(-3, 1) +
  theme_classic()

vary_eff_time |>
  filter(variable == "log_a2") |>
  ggplot(aes(y = mean, x = .join_data)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype="dashed") +
  ggtitle("log a2") +
  xlim(-3, 1) + ylim(-3, 1) +
  theme_classic()

ggplot(m.unif, aes(y = mean, x = .join_data)) +
  geom_raster(aes(fill = densities)) +
  scale_fill_gradient(low = "#00A0A001", high = "#00A0A07F") +
  geom_point(data = vary_eff_time |>
  filter(variable == "b1")) +
  geom_abline(intercept = 0, slope = 1, linetype="dashed") +
  ggtitle("b1") +
  xlim(120, 180) + ylim(120, 180) +
  theme_classic()

ggplot(m.unif, aes(y = mean, x = .join_data)) +
  geom_raster(aes(fill = densities)) +
  scale_fill_gradient(low = "#00A0A001", high = "#00A0A07F") +
  geom_point(data = vary_eff_time |>
  filter(variable == "b2")) +
  geom_abline(intercept = 0, slope = 1, linetype="dashed") +
  ggtitle("b2") +
  xlim(180, 250) + ylim(180, 250) +
  theme_classic()

```


# Testing our occupancy model with adequate distributions

For now, we used mostly uniformly distributed data for effort and date. However, in the data set, the distributions of both of these variables are NOT uniform across the range. Thus, we will test our model with generated date with the appropriate distribution of these variable. 

## Effort

Here is the distribution of efforts in the data :

```{r}
#| echo: false
#| warning: false

tar_load(obs_data)
obs_data <- obs_data[[1]]

ggplot(obs_data, aes(x = Nb_field_hours)) +
  geom_histogram() +
  theme_classic()

summary(obs_data$Nb_field_hours)

```

We will adapt our data-generating function to best approximate this distribution. We will use a normal distribution with a mean of 6 and a sd of 2.5. Such pdf looks like : 

```{r}
#| echo: false

x <- seq(0, 40, length.out = 100)
plot(dnorm(x, 6, 2.5) ~ x, type = "l")

```

Once the sampling is changed, we ran the model and look at the fit between the mean of the posterior and the real value.

```{r}
#| echo: false
#| column: page
#| warning: false

tar_load(vary_effN_time)

vary_effN_time |>
  filter(variable == "prob_detect") |>
  ggplot(aes(y = mean, x = .join_data)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype="dashed") +
  ggtitle("Probability of detection") +
  xlim(0, 0.75) + ylim(0, 0.75) +
  theme_classic()

```

```{r}
#| echo: false
#| layout-ncol: 2
#| column: page
#| warning: false

vary_effN_time |>
  filter(variable == "log_a1") |>
  ggplot(aes(y = mean, x = .join_data)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype="dashed") +
  ggtitle("log a1") +
  xlim(-3, 1) + ylim(-3, 1) +
  theme_classic()

vary_effN_time |>
  filter(variable == "log_a2") |>
  ggplot(aes(y = mean, x = .join_data)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype="dashed") +
  ggtitle("log a2") +
  xlim(-3, 1) + ylim(-3, 1) +
  theme_classic()

ggplot(m.unif, aes(y = mean, x = .join_data)) +
  geom_raster(aes(fill = densities)) +
  scale_fill_gradient(low = "#00A0A001", high = "#00A0A07F") +
  geom_point(data = vary_effN_time |>
  filter(variable == "b1")) +
  geom_abline(intercept = 0, slope = 1, linetype="dashed") +
  ggtitle("b1") +
  xlim(120, 180) + ylim(120, 180) +
  theme_classic()

ggplot(m.unif, aes(y = mean, x = .join_data)) +
  geom_raster(aes(fill = densities)) +
  scale_fill_gradient(low = "#00A0A001", high = "#00A0A07F") +
  geom_point(data = vary_effN_time |>
  filter(variable == "b2")) +
  geom_abline(intercept = 0, slope = 1, linetype="dashed") +
  ggtitle("b2") +
  xlim(180, 250) + ylim(180, 250) +
  theme_classic()
```


## Date

Here is the distribution of dates in the data :

```{r}
#| echo: false
#| warning: false

ggplot(obs_data, aes(x = Date)) +
  geom_histogram() +
  theme_classic()

summary(obs_data$Date)

```

As with the effort, we change the distribution of date in our data-generating function to best approximate the distribution in the data. We use a normal distribution with a mean of 180 and a sd of 15. The pdf looks like: 
```{r}
#| echo: false

x <- seq(130, 234, length.out = 100)
plot(dnorm(x, 180, 15) ~ x, type = "l")

```

Once the sampling is changed, we ran the model and look at the fit between the mean of the posterior and the real value.

```{r}
#| echo: false
#| column: page
#| warning: false

tar_load(vary_effN_timeN)

vary_effN_timeN |>
  filter(variable == "prob_detect") |>
  ggplot(aes(y = mean, x = .join_data)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype="dashed") +
  ggtitle("Probability of detection") +
  xlim(0, 0.75) + ylim(0, 0.75) +
  theme_classic()

```

```{r}
#| echo: false
#| layout-ncol: 2
#| column: page
#| warning: false

# Create density probability matrix for the sampling date distribution
dates <- c(110:250)
m.norm <- expand.grid(dates, dates)
names(m.norm) <- c(".join_data", "mean")
m.norm$densities <- dnorm(m.norm$.join_data, 180, 15)

vary_effN_timeN |>
  filter(variable == "log_a1") |>
  ggplot(aes(y = mean, x = .join_data)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype="dashed") +
  ggtitle("log a1") +
  xlim(-3, 1) + ylim(-3, 1) +
  theme_classic()

vary_effN_timeN |>
  filter(variable == "log_a2") |>
  ggplot(aes(y = mean, x = .join_data)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype="dashed") +
  ggtitle("log a2") +
  xlim(-3, 1) + ylim(-3, 1) +
  theme_classic()

ggplot(m.norm, aes(y = mean, x = .join_data)) +
  geom_raster(aes(fill = densities)) +
  scale_fill_gradient(low = "#00A0A001", high = "#00A0A0") +
  geom_point(data = vary_effN_timeN |>
  filter(variable == "b1")) +
  geom_abline(intercept = 0, slope = 1, linetype="dashed") +
  ggtitle("b1") +
  xlim(120, 180) + ylim(120, 180) +
  theme_classic()

  
ggplot(m.norm, aes(y  = mean, x = .join_data)) +
  geom_raster(aes(fill = densities)) +
  scale_fill_gradient(low = "#00A0A001", high = "#00A0A0") +
  geom_point(data = vary_effN_timeN |>
  filter(variable == "b2")) +
  geom_abline(intercept = 0, slope = 1, linetype="dashed") +
  ggtitle("b2") +
  xlim(170, 240) + ylim(170, 240) +
  theme_classic()
  
```

# Testing our occupancy model with data

## Data on Long-tailed Duck

Now we test our Bayesian framework on our real observation data. For this test, we select the data of the Long-tailed Duck as it is rather easy to observe, distributed almost everywhere on the island and should have an arrival and departure date within the observation period.

First, we illustrate the data :

```{r}
#| echo: false

tar_load(duck_data)

data.frame(
  "y" = duck_data$y,
  "jj_date" = duck_data$jj_date,
  "effort" = duck_data$effort
) |>
  mutate(effort_scale = effort/max(effort)) |> 
  ggplot(aes(x = jj_date, y = y)) +
  geom_jitter(height = 0.35, aes(alpha = effort_scale)) +
  theme_classic()


```

## Running the model, all years together

Then we run the model and extract the distribution of the posteriors :

```{r}
#| echo: false
#| warning: false

tar_load(duck_fixed_year)

posterior <- duck_fixed_year[[2]]

bayesplot::mcmc_areas_ridges(
     posterior, 
     pars = c("prob_detect"),
     border_size = 0.75,
     prob = 0.8, # 80% intervals
) + xlim(0, 1)

```

```{r}
#| echo: false
#| layout-ncol: 2
#| warning: false
#| column: page

bayesplot::mcmc_areas_ridges(
     posterior, 
     pars = c("log_a1"),
     border_size = 0.75,
     prob = 0.8, # 80% intervals
     transformations = list( "log_a1" = "exp")
) + xlim(0, 0.5)

bayesplot::mcmc_areas_ridges(
     posterior, 
     pars = c("log_a2"),
     border_size = 0.75,
     prob = 0.8, # 80% intervals
     transformations = list( "log_a2" = "exp")
) + xlim(0, 0.5)

bayesplot::mcmc_areas_ridges(
     posterior, 
     pars = c("b1"),
     border_size = 0.75,
     prob = 0.8, # 80% intervals
) + xlim(160, 170)

bayesplot::mcmc_areas_ridges(
     posterior, 
     pars = c("b2"),
     border_size = 0.75,
     prob = 0.8, # 80% intervals
) + xlim(195,210)

```

We can extract the posterior for each chain

```{r}
#| echo: false
#| warning: false
#| fig-align: center
#| column: page

bayesplot::mcmc_dens_overlay(posterior, 
                  pars = c("prob_detect", "log_a1", "log_a2", "b1", "b2"), 
                  transformations = list("log_a2" = "exp", "log_a1" = "exp"))

```

We can then extract the mean parameters and plot the predicted curve of presence :

```{r}
#| echo: false

duck_fixed_year[[1]]

```

```{r}
#| echo: false
#| warning: false

a1 = -1.19 
a2 = -2.73 
b1 = 162.2
b2 = 202.7

data.frame(
  "y" = duck_data$y,
  "jj_date" = duck_data$jj_date,
  "effort" = duck_data$effort
) |>
  group_by(y, jj_date) |> 
  summarise(n = n()) |> 
  ggplot(aes(x = jj_date, y = y)) +
  geom_point(alpha = 0.2, aes(size = n)) +
  theme_classic()  +
  stat_function(
    fun = function(x)
      (1 / (1 + exp(-exp(a1) * (
        x - b1
      )))) * (1 / (1 + exp(exp(a2) * (
        x - b2
      )))),
    color = "#00A0A07F",
    lwd = 1.5
  )
  

```


### Posterior distribution of mean and predicted observations

Below, we manually extracted the coefficients from the model to plot the predicted model. However, we can modify the Stan code to extract the posterior directly into the Stan computation. It uses the notation "general quantities", as shown below. 

```{r}
#| class-output: stan
#| echo: false
gq <- cmdstanr::cmdstan_model(stan_file = "occ_eff_time_gq.stan",
  pedantic = TRUE)

gq
```

He we plot the densities of $y$ in our data set versus the posterior. The model performs well as the observed $y$ density falls well within the posterior of the model.

```{r}
#| echo: false

tar_load(gq_post_mcmc_occ_eff_time_gq)

post_pred_dist <- gq_post_mcmc_occ_eff_time_gq$draws(variables = "obs_pred") |> 
  posterior::as_draws_matrix()
  
bayesplot::ppc_dens_overlay(y = duck_data$y, 
                            yrep = head(post_pred_dist, 200))
```

Try this again but make a smooth line for the average and plot against the julian date :

```{r}
#| echo: false

tar_load(gq_line_mcmc_occ_eff_time_line)
tar_load(new_dates)

gq_line_mcmc_occ_eff_time_line |> 
  tidybayes::gather_rvars(mu_line[i]) |> 
  mutate(date = new_dates[i]) |> 
  ggplot(aes(x = date, dist = .value)) + 
  tidybayes::stat_lineribbon() +
  theme_classic()

```

Not really surprising that the interval is bigger when the $a_i$ is lower (more variation, more uncertainty).


# Fixed effect occupancy model

The next step in our analysis is to slowly increase the complexity of the model. As we will want to estimate the parameters for all years and all species, we will need to transform our model into a hierarchical one (as we assume that for each species, the parameters have a certain distribution). Before doing that, we transform our model to account for fixed effect of year on our parameters (thus, they are estimated as factors, independently from each other). 

## One parameter varying with year


First, we have to simulate data. We are gonna start one parameter at the time and begin with $b_1$. We simulate 200 observations per year for 5 years. We also add the variable "year". The data looks like this :

```{r}
#| echo: false

tar_load(plot_fake_data_b1)
plot_fake_data_b1
```

We modify our Stan code so that a $b_1$ parameter is estimated for each year 

```{r}
#| class-output: stan
#| echo: false
#| error: false

b1_fixed_stan <- cmdstanr::cmdstan_model(stan_file = "b1_fixed.stan")

b1_fixed_stan
```

```{r}
#| echo: false

tar_load(b1_fixed_mod_mcmc_b1_fixed)
```

::: {.callout-note}
- For the code to work, I had to transform the 'year' variable into a factor (else had an error message about the fact that the variable was outside the bounded array)
- I can't pass the b1 vector in the .join data as the function demands that the length of the variable in .join to be the same (1 in this case)
:::

Inspect the distribution of our parameters posteriors

```{r}
#| echo: false

b1_fixed_mod_mcmc_b1_fixed$draws(variables = c("prob_detect", "log_a1", "log_a2", "b1", "b1")) |> 
  posterior::summarise_draws() |> 
  flextable::flextable()
```

True b1 values are

```{r}
#| echo: false
tar_load(fake_data_b1)
fake_data_b1$.join_data$b1
```


### Visualize

```{r}
#| echo: false
tar_load(fake_data_b1)

b1_fixed_mod_mcmc_b1_fixed |> 
  tidybayes::gather_rvars(mu_line[i]) |> 
  dplyr::bind_cols(as.data.frame(fake_data_b1[c("newdate", "newyear")])) |> 
  ggplot(aes(x = newdate, dist = .value, group = newyear)) + 
  tidybayes::stat_lineribbon() +
  theme_classic() +
  facet_wrap(~ newyear)
```


It is not really surprizing that the model still work as we just added data and different values of $b1$ for those slices of data. We could do this for all parameters, but it will end up with the same results that we had in our previous analysis with all parameters.

The next step is to test for all parameters.

## Two b parameter varying with year

```{r}
#| echo: false
#| error: false

tar_load(plot_fake_data_b1_b2)
plot_fake_data_b1_b2

```


```{r}
#| echo: false

tar_load(b1_b2_fixed_mod_mcmc_b1_b2_fixed)

b1_b2_fixed_mod_mcmc_b1_b2_fixed$draws(variables = c("prob_detect", "log_a1", "log_a2", "b1", "b2")) |> 
  posterior::summarise_draws() |> 
  flextable::flextable()
```

```{r}
#| echo: false
tar_load(fake_data_b1_b2)

b1_b2_fixed_mod_mcmc_b1_b2_fixed |> 
  tidybayes::gather_rvars(mu_line[i]) |> 
  dplyr::bind_cols(as.data.frame(fake_data_b1_b2[c("newdate", "newyear")])) |> 
  ggplot(aes(x = newdate, dist = .value, group = newyear)) + 
  tidybayes::stat_lineribbon() +
  theme_classic() +
  facet_wrap(~ newyear)
```

## One $a$ parameter varying with year

```{r}
#| echo: false
#| error: false
tar_load(plot_fake_data_a1)
plot_fake_data_a1

```

```{r}
#| echo: false

tar_load(a1_fixed_mod_mcmc_a1_fixed)

a1_fixed_mod_mcmc_a1_fixed$draws(variables = c("prob_detect", "log_a1", "log_a2", "b1", "b2")) |> 
  posterior::summarise_draws() |> 
  flextable::flextable()
```

```{r}
#| echo: false
tar_load(fake_data_b1_b2)

a1_fixed_mod_mcmc_a1_fixed |> 
  tidybayes::gather_rvars(mu_line[i]) |> 
  dplyr::bind_cols(as.data.frame(fake_data_b1_b2[c("newdate", "newyear")])) |> 
  ggplot(aes(x = newdate, dist = .value, group = newyear)) + 
  tidybayes::stat_lineribbon() +
  theme_classic() +
  facet_wrap(~ newyear)
```


## Two $a$ parameter varying with year

```{r}
#| echo: false
#| error: false

tar_load(plot_fake_data_a1_a2)
plot_fake_data_a1_a2

```

```{r}
#| echo: false

tar_load(a1_a2_fixed_mod_mcmc_a1_a2_fixed)

a1_a2_fixed_mod_mcmc_a1_a2_fixed$draws(variables = c("prob_detect", "log_a1", "log_a2", "b1", "b2")) |> 
  posterior::summarise_draws() |> 
  flextable::flextable()
```

At this point, we have a problem as the algorithm seems not able to distinguish the two $a$ parameter.

```{r}
#| echo: false
tar_load(fake_data_a1_a2)

a1_a2_fixed_mod_mcmc_a1_a2_fixed |> 
  tidybayes::gather_rvars(mu_line[i]) |> 
  dplyr::bind_cols(as.data.frame(fake_data_a1_a2[c("newdate", "newyear")])) |> 
  ggplot(aes(x = newdate, dist = .value, group = newyear)) + 
  tidybayes::stat_lineribbon() +
  theme_classic() +
  facet_wrap(~ newyear)
```


## Logit detour

To deal with the issue of 'misindentification' of the $a$ parameter, we modify the model. We keep only one $a$ parameter for both slopes. However, we add an other parameter $f$ that's gonna modify the value of both slope. The spring slope will thus be $a*f$. As we expect that the slope of fall to be lower, we multiply it by $(1-f)$. We set the prior of $f$ so that it is between 0.5 and 1 (which will cause a lower slope in fall). However, with it sd, we allow it to be under 0.5 (which would indicate a higher slope in spring). At a value of exactly 0.5, both slopes are equal (the curvature is symmetric).

Further, to deal with scale, we estimate the $a$ and $f$ parameters on the log and absolute scale. This is what the equation system now look like with the logit detour :

$$
\begin{align}
Pr(y = 1) &= Bernoulli(w * p) \\\\

w &= 1 - (1 - d)^{effort} \\
p &= \dfrac{e^{-a f (date - b_1)}}{1 + e^{-a f (date - b_1)}} * \dfrac{e^{a (1 - f) (date - b_2)}}{1+e^{a (1 - f) (date - b_2)}}\\\\ 

a &= exp(\alpha) \\
f &= \dfrac{e^{\phi}}{1 + e^{\phi}} \\\\

\alpha &\sim \text{Normal} \\
\phi &\sim \text{Normal}\\
b_1 &\sim \text{Normal}\\
b_2 &\sim \text{Normal}\\
\end{align}
$$

The modified stan code looks like : 

```{r}
#| class-output: stan
#| echo: false
#| error: false

a_logis_stan <- cmdstanr::cmdstan_model(stan_file = "a_logis.stan")

a_logis_stan
```

For this simulation, we will vary both parameter $a$ and $f$ each year. This means that the slopes should be different for each year and their relation (symmetry) should also differs.

```{r}
#| echo: false
#| error: false

tar_load(plot_fake_data_logis)
plot_fake_data_logis

```

We than run this model and check the distribution of the estimated parameters. They look just fine !!

```{r}
#| echo: false
tar_load(logis_a_mcmc_a_logis)

logis_a_mcmc_a_logis$draws(variables = c("prob_detect", "log_a", "f", "b1", "b2")) |> 
  posterior::summarise_draws() |> 
  flextable::flextable()
```

The prediction of our curve is also very good. We are gonna keep this approach for the 'complete' model, where all the parameters (so $b1$ and $b2$) vary with year. 

```{r}
#| echo: false
tar_load(fake_data_logis)

logis_a_mcmc_a_logis |> 
  tidybayes::gather_rvars(mu_line[i]) |> 
  dplyr::bind_cols(as.data.frame(fake_data_logis[c("newdate", "newyear")])) |> 
  ggplot(aes(x = newdate, dist = .value, group = newyear)) + 
  tidybayes::stat_lineribbon() +
  theme_classic() +
  facet_wrap(~ newyear)
```

## All parameters

As for before, we start to generate fake data. For each year, we will make the following parameters vary : $a$, $f$, $b1$ and $b2$. The only parameter that we keep constant is the probability of detection, as we assume that it should no vary, for one species, between years. 
With parameters varying each year, we modify the equation so that it matches for year $i$
$$
\begin{align}
Pr(y = 1) &= Bernoulli(w * p) \\\\

w &= 1 - (1 - d)^{effort} \\
p &= \dfrac{e^{-a^i f^i (date - b_1^i)}}{1 + e^{-a^i f^i (date - b_1^i)}} * \dfrac{e^{a^i (1 - f^i) (date - b_2^i)}}{1+e^{a^i (1 - f^i) (date - b_2^i)}}\\\\ 

a^i &= exp(\alpha^i) \\
f^i &= \dfrac{e^{\phi^i}}{1 + e^{\phi^i}} \\\\

\alpha^i &\sim \text{Normal} \\
\phi^i &\sim \text{Normal}\\
b_1^i &\sim \text{Normal}\\
b_2^i &\sim \text{Normal}\\
\end{align}
$$

```{r}
#| echo: false

tar_load(fake_data_all)
str(fake_data_all)
```

```{r}
#| echo: false
#| error: false
tar_load(plot_fake_data_all)
plot_fake_data_all

```

```{r}
#| echo: false

tar_load(all_fixed_mod_mcmc_all_fixed)

all_fixed_mod_mcmc_all_fixed$draws(variables = c("prob_detect", "log_a", "f", "b1", "b2")) |> 
  posterior::summarise_draws() |> 
  flextable::flextable()
```

```{r}
#| echo: false
tar_load(fake_data_all)

all_fixed_mod_mcmc_all_fixed |> 
  tidybayes::gather_rvars(mu_line[i]) |> 
  dplyr::bind_cols(as.data.frame(fake_data_all[c("newdate", "newyear")])) |> 
  ggplot(aes(x = newdate, dist = .value, group = newyear)) + 
  tidybayes::stat_lineribbon() +
  theme_classic() +
  facet_wrap(~ newyear)
```

# Testing the ~Year model with data

## Vizualising the data
```{r}
#| echo: false
#| column: screen-inset
#| fig-align: center
#| fig-width: 8
#| fig-height: 8

tar_load(duck_data_all)

duck_data_all[c(3, 4, 6)] |> 
  as.data.frame() |> 
  group_by(y, year, jj_date) |> 
  count() |> 
  ggplot(aes(x = jj_date, y = y)) +
  geom_point(alpha = 0.2, aes(size = n)) +
  theme_classic() +
  facet_wrap(~ year)

```

## Run the model

We will apply our model to the long-tailed duck data. Estimating parameters for each years.

```{r}
#| echo: false
#| column: screen-inset
tar_load(duck_all_mod_mcmc_all_fixed)

duck_all_mod_mcmc_all_fixed$draws(variables = c("prob_detect", "log_a", "f", "b1", "b2")) |> 
  posterior::summarise_draws() |> 
  flextable::flextable()
```

```{r}
#| echo: false
#| column: screen-inset
#| fig-align: center
#| fig-width: 10
#| fig-height: 10

tar_load(duck_data_all)

duck_all_mod_mcmc_all_fixed |> 
  tidybayes::gather_rvars(mu_line[i]) |> 
  dplyr::bind_cols(as.data.frame(duck_data_all[c("newdate", "newyear")])) |> 
  ggplot(aes(x = newdate, dist = .value, group = newyear)) + 
  tidybayes::stat_lineribbon() +
  theme_classic() +
  facet_wrap(~ newyear)
```

## Explore the results

Plot the arrival and departure date

```{r}
#| echo: false

duck.year <- duck_all_mod_mcmc_all_fixed$draws(variables = c("b1", "b2"))  |> 
  posterior::summarise_draws() |> 
  select(variable, mean, q5, q95) |> 
  mutate(Year = rep(c(2007:2019, 2021, 2022), 2))


duck.segment <- cbind(duck.year[16:30, c(5, 2)], duck.year[1:15, 2])
names(duck.segment) <- c("Year", "B1", "B2")
duck.segment <- duck.segment |> 
  mutate(duration = B2 - B1) |> 
  filter(duration > 0)

ggplot(duck.year, aes(x = Year, y = mean, colour = factor(Year))) +
  geom_segment(data = duck.segment, mapping = aes(x = Year, xend = Year, y = B1, yend = B2), color="grey", linetype = "dashed") +
  geom_point(size = 3.5, show.legend = FALSE) +
  geom_errorbar(aes(ymin = q5, ymax = q95), width = 0.5, linewidth = 1, show.legend = FALSE) +
  viridis::scale_color_viridis(discrete = TRUE) + 
  coord_flip() + theme_classic() 

```

Histogram of length of stay 

```{r}
#| echo: false

ggplot(duck.segment, aes(x = Year, y = duration)) +
  geom_line(linewidth = 0.8, colour = "grey", linetype = "dashed") +
  geom_point(size = 3.5, aes(colour = factor(Year)), show.legend = FALSE) + 
  viridis::scale_color_viridis(discrete = TRUE) +
  theme_classic()

```


# Hierarchical model

The next step is to consider that each parameter of the model is nested in a distribution of said parameters: a hierarchical model ! We do this because for some year, the data collection started later than for others. With no apparent trend in the arrival/departure date, we will capitalize on the information of other years to inform the years with less data. This will be particularly useful for estimating the departure date, as some species seems to leave after us (ex: geese, crane and loons). 

## Hierarchical model for one parameter

We start to test the structure of our Stan model with only one parameter (here $b_1$). We modify the equation so that $b_1$ is nested in a distribution with it's own mean and variance.

$$
\begin{align}
Pr(y = 1) &= Bernoulli(w * p) \\\\

w &= 1 - (1 - d)^{effort} \\
p &= \dfrac{e^{-a^i f^i (date - b_1^i)}}{1 + e^{-a^i f^i (date - b_1^i)}} * \dfrac{e^{a^i (1 - f^i) (date - b_2^i)}}{1+e^{a^i (1 - f^i) (date - b_2^i)}}\\\\ 

a^i &= exp(\alpha^i) \\
f^i &= \dfrac{e^{\phi^i}}{1 + e^{\phi^i}} \\\\

\alpha^i &\sim \text{Normal} \\
\phi^i &\sim \text{Normal}\\
b_1^i &\sim \text{Normal}\\
b_2^i &\sim \text{Normal}\\
\end{align}
$$

### Create a false data set

We create the same false data set, but this time we constrain $b1$ in a distribution. We will use a normal distribution centered on 150 with a sd of 2.5 (which approximately means a range of 1 week).

For computational simplicity, we sampled all the parameters (except probability of detection) in a normal distribution with given mean and variance. Thus, we will use the same hierarchical fake data set for all model test.

```{r}
#| echo: false
#| column: page
#| fig-align: center

tar_load(plot_fake_data_hierarchical)

plot_fake_data_hierarchical

```

Her is the pdf of all the parameters

```{r}
#| echo: false
#| layout-nrow: 2
#| column: page

tar_load(fake_data_hierarchical)

x1 <- seq(-3, 1, length.out = 100)
x2 <- seq(-1, 3, length.out = 100)
x3 <- seq(140, 160, length.out = 100)
x4 <- seq(190, 210, length.out = 100)

plot(x1, dnorm(x1, mean = fake_data_hierarchical$.join_data$hyperparameters$a.mu, 
                  sd = fake_data_hierarchical$.join_data$hyperparameters$a.sigma), type = "l", xlab = "", yaxt="n", ylab="", lwd = 2, main = "a")
abline(v = fake_data_hierarchical$.join_data$a, col = viridis::viridis(5)[1], lwd = 2)


plot(x2, dnorm(x2, mean = fake_data_hierarchical$.join_data$hyperparameters$f.mu, 
                  sd = fake_data_hierarchical$.join_data$hyperparameters$f.sigma), type = "l", xlab = "", yaxt="n", ylab="", lwd = 2, main = "f")
abline(v = fake_data_hierarchical$.join_data$f, col = viridis::viridis(5)[2], lwd = 2)


plot(x3, dnorm(x3, mean = fake_data_hierarchical$.join_data$hyperparameters$b1.mu, 
                  sd = fake_data_hierarchical$.join_data$hyperparameters$b1.sigma), type = "l", xlab = "", yaxt="n", ylab="", lwd = 2, main = "b1")
abline(v = fake_data_hierarchical$.join_data$b1, col = viridis::viridis(5)[3], lwd = 2)


plot(x4, dnorm(x4, mean = fake_data_hierarchical$.join_data$hyperparameters$b2.mu, 
                  sd = fake_data_hierarchical$.join_data$hyperparameters$b2.sigma), type = "l", xlab = "", yaxt="n", ylab="", lwd = 2, main = "b2")
abline(v = fake_data_hierarchical$.join_data$b2, col = viridis::viridis(5)[4], lwd = 2)

```

** **TODO** **


- [x] Understand why the model with presence ~ time does not seems to fit very well
- [x] Run the model with presence parameters varying at each simulation
- [x] Build the 1:1 graph for the mean of the posterior vs real parameters
- [X] Run a model for the Long-tailed Duck data all year combined
- [X] Illustrate the model like in [this article](https://mc-stan.org/bayesplot/articles/plotting-mcmc-draws.html)
- [X] Adjust the model for a logit form
- [x] Simulate and run an additive model in Stan
- [X] Simulate and run a model with all the relevant parameter varying with year
- [X] Run a model for the Long-tailed Duck with parameter varying each year
- [ ] Simulate data with a hierarchical structure for one parameters (ex: b1)
- [ ] Run a hierarchical model for one parameters
